{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA TRABALHAR COM O GOOGLE COLAB É NECESSÁRIO:\n",
    "# !apt-get update -qq\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: para este projeto estamos trabalhando com o google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "SPARK = spark.read.json('/content/drive/ml/imoveis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAR COLUNAS MULTICATEGORICAS STRING PARA BINÁRIOS COM PIVOT (DUMMIES)\n",
    "SPARK.groupBy('ID_UNICO'.pivot('COLUNA_2')).agg.(f.lit(1)).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA UTILIZAR ML COM SPARK, NECESSITAMOS VETORIZAR O DATAFRAME\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importante: Para o modelo de regressão é esperado que o valor a ser previsto tenha o nome \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK = SPARK.withColumnRenamed('COLUNA_2', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAMOS SEPARAR EM VETORES VARIAVEIS EXPLICATIVAS\n",
    "X = ['COLUNA_5',\n",
    "    'COLUNA_6',\n",
    "    'COLUNA_7',\n",
    "    'COLUNA_8',\n",
    "    'COLUNA_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A VETORIZAÇÃO É FEITA PELO OBJETO VETOR E AS FUNÇÕIES DO SPARK ESPERAM RECEBER O OUTPUT CO0M NOME 'features'\n",
    "ASSEMBLER = VectorAssembler(inputCols=X, outputCol='features')\n",
    "\n",
    "DF_PREP = ASSEMBLER.transform(SPARK).select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# TRABALHANDO APENAS COM O METODO PEARSON (MATRIZES DENSAS)\n",
    "CORRELACAO = Correlation.corr(DF_PREP, 'features').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMANDO EM VETOR PARA TRABALHAR DE FORMA MAIS FACIL\n",
    "# CRIANDO MATRIZ DE CORRELAÇÃO EM UM DATAFRAME DO PANDAS\n",
    "DF_CORRELACAO = pd.DataFrame(CORRELACAO.toArray(), columns=X, index=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA FACILITAR A VISUALIZAÇÃO DOS DADOS (PRINCIPALMENTE DE SUA GRANDEZA)\n",
    "# VAMOS CRIAR UM HEATMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "paleta = sns.color_palette(\"light:salmon\", as_cmap=True)\n",
    "sns.heatmap(DF_CORRELACAO.round(1), annot=True, cmap=paleta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# DEFININDO AS VERIAVEIS DE TREINO E TESTE PARA 0.7/0.3 E DEIXANDO A ALEATORIEDADE COM UM SEED MAIS ESPECIFICO,\n",
    "# GARANTINDO QUE TEREMOS O MESMO RESULTADO SEMPRE QUE RODARMOS\n",
    "TREINO, TESTE = DF_PREP.randomSplit([0.7, 0.3], seed=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSÃO UTILIZANDO O MÉTODO REGRESSÃO LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "MODELO_LR = lr.fit(TREINO)\n",
    "PREVISAO_LR_TREINO = MODELO_LR.transform(TREINO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUMO = MODELO_LR.summary\n",
    "\n",
    "# AGORA TEMOS QUE ANALISAR O AJUSTE DO MODELO AOS DADOS (R2)\n",
    "AJUSTE = RESUMO.r2\n",
    "\n",
    "# E TAMBÉM O ERRO ASSOCIADO (RMSE) - QUANTO MENOR ESTA METRICA, MELHOR\n",
    "ERRO = RESUMO.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGORA AVALIAREMOS (EVALUATE) COM O TESTE\n",
    "RESUMO_TESTE = MODELO_LR.evaluate(TESTE)\n",
    "\n",
    "AJUSTE_TESTE = RESUMO_TESTE.r2\n",
    "ERRO_TESTE = RESUMO_TESTE.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSÃO UTILIZANDO O MÉTODO DE ARVORE DE DECISÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# ALTURA ARBITRARIA PARA ARVORE \"maxDepth\"\n",
    "ARVORE = DecisionTreeRegressor(seed=101, maxDepth=7)\n",
    "\n",
    "MODELO_ARVORE = ARVORE.fit(TREINO)\n",
    "PREVISAO_ARVORE_TREINO = MODELO_ARVORE.transform(TREINO)\n",
    "PREVISAO_ARVORE_TREINO.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGORA PRECISAMOS AVALIAR O MODELO\n",
    "# DESSA VEZ VAMOS UTILIZAR UM OBJ DA LIB DO PYSPARK\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "EVALUATOR = RegressionEvaluator()\n",
    "print(EVALUATOR.evaluate(PREVISAO_ARVORE_TREINO, {EVALUATOR.merticName: \"r2\"}))\n",
    "print(EVALUATOR.evaluate(PREVISAO_ARVORE_TREINO, {EVALUATOR.merticName: \"rmse\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVALIANDO O TESTE PARA A ARVORE\n",
    "PREVISAO_ARVORE_TESTE = MODELO_ARVORE.transform(TESTE)\n",
    "\n",
    "print(EVALUATOR.evaluate(PREVISAO_ARVORE_TESTE, {EVALUATOR.merticName: \"r2\"}))\n",
    "print(EVALUATOR.evaluate(PREVISAO_ARVORE_TESTE, {EVALUATOR.merticName: \"rmse\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSÃO UTILIZANDO O MÉTODO RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "RFR = RandomForestRegressor(seed=101, maxDepth=7, numTrees=10)\n",
    "MODELO_RFR = RFR.fit(TREINO)\n",
    "PREVISAO_RFR_TREINO = MODELO_RFR.transform(TREINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EVALUATOR.evaluate(PREVISAO_RFR_TREINO, {EVALUATOR.merticName: \"r2\"}))\n",
    "print(EVALUATOR.evaluate(PREVISAO_RFR_TREINO, {EVALUATOR.merticName: \"rmse\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVALIANDO TESTES DA RANDOM FOREST REGRESSION\n",
    "PREVISAO_RFR_TESTE = MODELO_RFR.transform(TESTE)\n",
    "\n",
    "print(EVALUATOR.evaluate(PREVISAO_RFR_TESTE, {EVALUATOR.merticName: \"r2\"}))\n",
    "print(EVALUATOR.evaluate(PREVISAO_RFR_TESTE, {EVALUATOR.merticName: \"rmse\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÉCNICAS DE OTIMIZAÇÃO: CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA O MÉTODO DE ARVORE DE DECISÃO\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#########################################################################\n",
    "###       O CROSS VALIDATOR NOS PERMITE FAZER DIVERSAS SEPARAÇÕES     ###\n",
    "###       DE TREINO/TESTE PARA AVALIAR/TREINAR MELHOR O MODELO        ###\n",
    "###      E O PARAMGRIDBUILDER É RESPONSAVEL NOS DEIXA AVALIAR MAIS    ###\n",
    "###    PARAMETROS DE UMA VEZ PARA ENTENDERMOS QUAL O MELHOR PARAMETRO ###\n",
    "###                         PARA OS MODELOS                           ###\n",
    "#########################################################################\n",
    "\n",
    "DTR = DecisionTreeRegressor()\n",
    "EVALUATOR = RegressionEvaluator()\n",
    "\n",
    "GRID = ParamGridBuilder().addGrid(DTR.maxDepth, [2, 5, 10]).addGrid(DTR.maxBins, [10, 32, 45]).build()\n",
    "DTR_CV = CrossValidator(estimator=DTR, estimatorParamMaps=GRID, evaluator=EVALUATOR, numFolds=3, seed=101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELO_DTR_CV = DTR_CV.fit(TREINO)\n",
    "PREVISOES_DTR_CV_TESTE = MODELO_DTR_CV.transform(TESTE)\n",
    "\n",
    "print(EVALUATOR.evaluate(PREVISOES_DTR_CV_TESTE, {EVALUATOR.merticName: \"r2\"}))\n",
    "print(EVALUATOR.evaluate(PREVISOES_DTR_CV_TESTE, {EVALUATOR.merticName: \"rmse\"}))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
