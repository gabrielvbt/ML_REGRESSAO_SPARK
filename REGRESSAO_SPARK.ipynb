{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA TRABALHAR COM O GOOGLE COLAB É NECESSÁRIO:\n",
    "# !apt-get update -qq\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: para este projeto estamos trabalhando com o google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "SPARK = spark.read.json('/content/drive/ml/imoveis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAR COLUNAS MULTICATEGORICAS STRING PARA BINÁRIOS COM PIVOT (DUMMIES)\n",
    "SPARK.groupBy('ID_UNICO'.pivot('COLUNA_2')).agg.(f.lit(1)).na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA UTILIZAR ML COM SPARK, NECESSITAMOS VETORIZAR O DATAFRAME\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importante: Para o modelo de regressão é esperado que o valor a ser previsto tenha o nome \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK = SPARK.withColumnRenamed('COLUNA_2', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAMOS SEPARAR EM VETORES VARIAVEIS EXPLICATIVAS\n",
    "X = ['COLUNA_5',\n",
    "    'COLUNA_6',\n",
    "    'COLUNA_7',\n",
    "    'COLUNA_8',\n",
    "    'COLUNA_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A VETORIZAÇÃO É FEITA PELO OBJETO VETOR E AS FUNÇÕIES DO SPARK ESPERAM RECEBER O OUTPUT CO0M NOME 'features'\n",
    "ASSEMBLER = VectorAssembler(inputCols=X, outputCol='features')\n",
    "\n",
    "DF_PREP = ASSEMBLER.transform(SPARK).select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# TRABALHANDO APENAS COM O METODO PEARSON (MATRIZES DENSAS)\n",
    "CORRELACAO = Correlation.corr(DF_PREP, 'features').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMANDO EM VETOR PARA TRABALHAR DE FORMA MAIS FACIL\n",
    "# CRIANDO MATRIZ DE CORRELAÇÃO EM UM DATAFRAME DO PANDAS\n",
    "DF_CORRELACAO = pd.DataFrame(CORRELACAO.toArray(), columns=X, index=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA FACILITAR A VISUALIZAÇÃO DOS DADOS (PRINCIPALMENTE DE SUA GRANDEZA)\n",
    "# VAMOS CRIAR UM HEATMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "paleta = sns.color_palette(\"light:salmon\", as_cmap=True)\n",
    "sns.heatmap(DF_CORRELACAO.round(1), annot=True, cmap=paleta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# DEFININDO AS VERIAVEIS DE TREINO E TESTE PARA 0.7/0.3 E DEIXANDO A ALEATORIEDADE COM UM SEED MAIS ESPECIFICO,\n",
    "# GARANTINDO QUE TEREMOS O MESMO RESULTADO SEMPRE QUE RODARMOS\n",
    "TREINO, TESTE = DF_PREP.randomSplit([0.7, 0.3], seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "MODELO_LR = lr.fit(TREINO)\n",
    "PREVISAO_LR_TREINO = MODELO_LR.transform(TREINO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUMO = MODELO_LR.summary\n",
    "\n",
    "# AGORA TEMOS QUE ANALISAR O AJUSTE DO MODELO AOS DADOS (R2)\n",
    "AJUSTE = RESUMO.r2\n",
    "\n",
    "# E TAMBÉM O ERRO ASSOCIADO (RMSE) - QUANTO MENOR ESTA METRICA, MELHOR\n",
    "ERRO = RESUMO.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGORA AVALIAREMOS (EVALUATE) COM O TESTE\n",
    "RESUMO_TESTE = MODELO_LR.evaluate(TESTE)\n",
    "\n",
    "AJUSTE_TESTE = RESUMO_TESTE.r2\n",
    "ERRO_TESTE = RESUMO_TESTE.rootMeanSquaredError"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
